{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dff473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78562f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c9bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa60a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import RegexTokenizer\n",
    "\n",
    "tokenizer = RegexTokenizer()\n",
    "tokenizer.load(model_file=\"../output/tokenizer/darija_tokenizer.model\")\n",
    "\n",
    "\n",
    "def get_vocab_size(tokenizer: RegexTokenizer) -> int:\n",
    "    vocab = tokenizer.vocab\n",
    "    special_tokens = tokenizer.special_tokens\n",
    "\n",
    "    return len(vocab) + len(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c70383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(3647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d0d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.model import GPTLanguageModel\n",
    "\n",
    "block_size = 1024\n",
    "n_embd = 512\n",
    "n_head = 12\n",
    "n_layer = 8\n",
    "dropout = 0.2\n",
    "batch_size = 4\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    dropout=dropout,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30223335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "data = np.load('../output/encoded_data/encoded_atlaset.npy', mmap_mode='r')\n",
    "print('Data shape:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9569a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(0.9*len(data))\n",
    "split_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c667dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def get_batch_for_loss_estimation(split:str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if split == 'train':\n",
    "        start_index = 0\n",
    "        end_index = split_index\n",
    "    else:\n",
    "        start_index = split_index\n",
    "        end_index = len(data)\n",
    "\n",
    "    available_blocks = (end_index - start_index -1) // block_size\n",
    "    block_indices = torch.randint(0, available_blocks, (batch_size,))\n",
    "\n",
    "    x_batch, y_batch = [], []\n",
    "    for i in block_indices:\n",
    "        block_start = start_index + (i * block_size)\n",
    "        x_batch.append(data[block_start:block_start+block_size])\n",
    "        y_batch.append(data[block_start+1:block_start+block_size+1])\n",
    "\n",
    "    x_batch = np.array(x_batch)\n",
    "    y_batch = np.array(y_batch)\n",
    "\n",
    "    x_batch = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
    "    y_batch = torch.tensor(y_batch, dtype=torch.long).to(device)\n",
    "\n",
    "    return x_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38123cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss() -> Dict:\n",
    "    output = {}\n",
    "    eval_iters = 1000\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch_for_loss_estimation(split)\n",
    "            _, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        output[split] = losses.mean()\n",
    "        model.train()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f66bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model: GPTLanguageModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    file_path: str = \"checkpoint.pth\"\n",
    ") -> None:\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fccecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 1024\n",
    "gradient_accumulation_steps = 8\n",
    "eval_interval = 100\n",
    "save_interval = 10000\n",
    "\n",
    "total_data_to_process = split_index - block_size\n",
    "total_data_to_process_in_batches = total_data_to_process // batch_size\n",
    "\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "batches_processed = 0\n",
    "train_losses, val_losses = [],[]\n",
    "\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "for i in tqdm(\n",
    "    iterable=range(0, total_data_to_process, batch_size),\n",
    "    desc=\"Processing\",\n",
    "    total=total_data_to_process_in_batches\n",
    "):\n",
    "    x_batch, y_batch = [], []\n",
    "    for j in range(i, i+batch_size):\n",
    "        x_batch.append(data[j:j+block_size])\n",
    "        y_batch.append(data[j+1:j+block_size+1])\n",
    "    \n",
    "    x_batch = np.array(x_batch)\n",
    "    y_batch = np.array(y_batch)\n",
    "\n",
    "    x_batch = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
    "    y_batch = torch.tensor(y_batch, dtype=torch.long).to(device)\n",
    "\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "    loss /= gradient_accumulation_steps\n",
    "    loss.backward()\n",
    "\n",
    "    batches_processed +=1\n",
    "    if batches_processed % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(\n",
    "            f\"Batch {batches_processed}: \"\n",
    "            f\"train loss {losses['train']:.4f}, \"\n",
    "            f\"val loss {losses['val']:.4f}\"\n",
    "        )\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "    \n",
    "    if batches_processed % save_interval == 0:\n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            epoch=batches_processed,\n",
    "            loss=loss.item(),\n",
    "            file_path=f\"../output/pre_training/run_1/checkpoint_{batches_processed}.pth\"\n",
    "        )\n",
    "\n",
    "if batches_processed % gradient_accumulation_steps != 0:\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "eval_interval = 3000\n",
    "save_interval = 10000\n",
    "\n",
    "# Calculate the number of complete non-overlapping blocks\n",
    "non_overlapping_blocks = (split_index - 1) // block_size\n",
    "total_batches = non_overlapping_blocks // batch_size\n",
    "\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "batches_processed = 0\n",
    "train_losses, val_losses = [], []\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "for i in tqdm(\n",
    "    iterable=range(0, non_overlapping_blocks, batch_size),\n",
    "    desc=\"Processing\",\n",
    "    total=total_batches\n",
    "):\n",
    "    # Load a batch of non-overlapping blocks\n",
    "    x_batch, y_batch = [], []\n",
    "    for j in range(batch_size):\n",
    "        if i+j < non_overlapping_blocks:\n",
    "            block_start = (i+j) * block_size\n",
    "            x = data[block_start:block_start+block_size]\n",
    "            y = data[block_start+1:block_start+block_size+1]\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "\n",
    "    if len(x_batch) == 0:\n",
    "        continue\n",
    "\n",
    "    x_batch = np.array(x_batch)\n",
    "    y_batch = np.array(y_batch)\n",
    "\n",
    "    x_batch = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
    "    y_batch = torch.tensor(y_batch, dtype=torch.long).to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "    loss /= gradient_accumulation_steps\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient accumulation\n",
    "    batches_processed += 1\n",
    "    if batches_processed % gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Evaluate the model\n",
    "    if batches_processed % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(\n",
    "            f\"Batch {batches_processed}: \"\n",
    "            f\"train loss {losses['train']:.4f}, \"\n",
    "            f\"val loss {losses['val']:.4f}\"\n",
    "        )\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "\n",
    "    # Save the model\n",
    "    if batches_processed % save_interval == 0:\n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            epoch=batches_processed,\n",
    "            loss=loss.item(),\n",
    "            file_path=f\"../output/pre_training/run_1/checkpoint_{batches_processed}.pth\"\n",
    "        )\n",
    "\n",
    "if batches_processed % gradient_accumulation_steps != 0:\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "save_checkpoint(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epoch=batches_processed,\n",
    "    loss=loss.item(),\n",
    "    file_path=f\"../output/pre_training/run_1/checkpoint_{batches_processed}.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b3ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Evaluation Step\")\n",
    "plt.ylim(0)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de81fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer.encode(\"hello\")\n",
    "input_tokens = torch.tensor(\n",
    "    input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_tokens=input_tokens, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e13dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
